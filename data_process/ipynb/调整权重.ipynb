{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def print_h5_structure(file_path):\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        def print_name(name, obj):\n",
    "            if isinstance(obj, h5py.Group):\n",
    "                print(f\"Group: {name}\")\n",
    "            elif isinstance(obj, h5py.Dataset):\n",
    "                print(f\"Dataset: {name}, Shape: {obj.shape}\")\n",
    "        f.visititems(print_name)\n",
    "\n",
    "print_h5_structure('/mnt/d/Research/PHD/DLEPS/data/zinc_vae_grammar_L56_E100_val.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_h5_structure('/mnt/d/Research/PHD/DLEPS/data/adjusted_weights.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adjust_weights.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting weights for: conv_1/conv_1_W_1:0\n",
      "Adjusting weights for: conv_2/conv_2_W_1:0\n",
      "Adjusting weights for: conv_3/conv_3_W_1:0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to synchronously create dataset (name already exists)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 130\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights adjustment completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# 使用示例：\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m \u001b[43madjust_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/mnt/d/Research/PHD/DLEPS/data/zinc_vae_grammar_L56_E100_val.hdf5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/mnt/d/Research/PHD/DLEPS/data/adjusted_weights.hdf5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m    133\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 60\u001b[0m, in \u001b[0;36madjust_weights\u001b[0;34m(weights_file, adjusted_weights_file)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_U_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m dataset_name:\n\u001b[1;32m     59\u001b[0m     new_weight_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecurrent_kernel:0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 60\u001b[0m     \u001b[43mlayer_group_out\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_weight_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_b_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m dataset_name:\n\u001b[1;32m     62\u001b[0m     new_weight_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias:0\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/dleps/lib/python3.10/site-packages/h5py/_hl/group.py:183\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    180\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    181\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[0;32m--> 183\u001b[0m dsid \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_new_dset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "File \u001b[0;32m~/anaconda3/envs/dleps/lib/python3.10/site-packages/h5py/_hl/dataset.py:165\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0, fill_time)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     sid \u001b[38;5;241m=\u001b[39m h5s\u001b[38;5;241m.\u001b[39mcreate_simple(shape, maxshape)\n\u001b[0;32m--> 165\u001b[0m dset_id \u001b[38;5;241m=\u001b[39m \u001b[43mh5d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdcpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Empty)):\n\u001b[1;32m    168\u001b[0m     dset_id\u001b[38;5;241m.\u001b[39mwrite(h5s\u001b[38;5;241m.\u001b[39mALL, h5s\u001b[38;5;241m.\u001b[39mALL, data)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5d.pyx:136\u001b[0m, in \u001b[0;36mh5py.h5d.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to synchronously create dataset (name already exists)"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def convert_conv1d_weights(weights):\n",
    "    if len(weights.shape) == 4:\n",
    "        # 旧的 Conv1D 权重形状: (kernel_size, 1, input_dim, filters)\n",
    "        # 新的 Conv1D 权重形状: (kernel_size, input_dim, filters)\n",
    "        weights = np.squeeze(weights, axis=1)\n",
    "    return weights\n",
    "\n",
    "def adjust_weights(weights_file, adjusted_weights_file):\n",
    "    with h5py.File(weights_file, 'r') as f_in, h5py.File(adjusted_weights_file, 'w') as f_out:\n",
    "        # 获取 'model_weights' 组\n",
    "        model_weights_group = f_in.get('model_weights')\n",
    "        if model_weights_group is None:\n",
    "            print(\"No 'model_weights' group found in the weights file.\")\n",
    "            return\n",
    "        \n",
    "        # 遍历 'model_weights' 组中的所有子组（即各层）\n",
    "        for layer_name in model_weights_group:\n",
    "            if layer_name.startswith('conv'):\n",
    "                # 处理 Conv1D 层\n",
    "                layer_group_in = model_weights_group[layer_name]\n",
    "                layer_group_out = f_out.create_group(layer_name)\n",
    "                for dataset_name in layer_group_in:\n",
    "                    data = layer_group_in[dataset_name][()]\n",
    "                    if '_W_' in dataset_name:\n",
    "                        print(f\"Adjusting weights for: {layer_name}/{dataset_name}\")\n",
    "                        data = convert_conv1d_weights(data)\n",
    "                        new_weight_name = 'kernel:0'\n",
    "                    elif '_b_' in dataset_name:\n",
    "                        new_weight_name = 'bias:0'\n",
    "                    else:\n",
    "                        new_weight_name = dataset_name\n",
    "                    layer_group_out.create_dataset(new_weight_name, data=data)\n",
    "            elif layer_name.startswith('dense'):\n",
    "                # 处理 Dense 层\n",
    "                layer_group_in = model_weights_group[layer_name]\n",
    "                layer_group_out = f_out.create_group(layer_name)\n",
    "                for dataset_name in layer_group_in:\n",
    "                    data = layer_group_in[dataset_name][()]\n",
    "                    if '_W_' in dataset_name:\n",
    "                        new_weight_name = 'kernel:0'\n",
    "                    elif '_b_' in dataset_name:\n",
    "                        new_weight_name = 'bias:0'\n",
    "                    else:\n",
    "                        new_weight_name = dataset_name\n",
    "                    layer_group_out.create_dataset(new_weight_name, data=data)\n",
    "            elif layer_name.startswith('gru'):\n",
    "                # 处理 GRU 层\n",
    "                layer_group_in = model_weights_group[layer_name]\n",
    "                layer_group_out = f_out.create_group(layer_name)\n",
    "                for dataset_name in layer_group_in:\n",
    "                    data = layer_group_in[dataset_name][()]\n",
    "                    if '_W_' in dataset_name:\n",
    "                        new_weight_name = 'kernel:0'\n",
    "                        layer_group_out.create_dataset(new_weight_name, data=data)\n",
    "                    elif '_U_' in dataset_name:\n",
    "                        new_weight_name = 'recurrent_kernel:0'\n",
    "                        layer_group_out.create_dataset(new_weight_name, data=data)\n",
    "                    elif '_b_' in dataset_name:\n",
    "                        new_weight_name = 'bias:0'\n",
    "                        layer_group_out.create_dataset(new_weight_name, data=data)\n",
    "                    else:\n",
    "                        new_weight_name = dataset_name\n",
    "                        layer_group_out.create_dataset(new_weight_name, data=data)\n",
    "            elif layer_name.startswith('latent_input'):\n",
    "                # 处理 latent_input 层\n",
    "                layer_group_in = model_weights_group[layer_name]\n",
    "                layer_group_out = f_out.create_group(layer_name)\n",
    "                for dataset_name in layer_group_in:\n",
    "                    data = layer_group_in[dataset_name][()]\n",
    "                    if '_W_' in dataset_name:\n",
    "                        new_weight_name = 'kernel:0'\n",
    "                    elif '_b_' in dataset_name:\n",
    "                        new_weight_name = 'bias:0'\n",
    "                    else:\n",
    "                        new_weight_name = dataset_name\n",
    "                    layer_group_out.create_dataset(new_weight_name, data=data)\n",
    "            elif layer_name.startswith('z_mean') or layer_name.startswith('z_log_var'):\n",
    "                # 处理 z_mean 和 z_log_var 层\n",
    "                layer_group_in = model_weights_group[layer_name]\n",
    "                layer_group_out = f_out.create_group(layer_name)\n",
    "                for dataset_name in layer_group_in:\n",
    "                    data = layer_group_in[dataset_name][()]\n",
    "                    if '_W_' in dataset_name:\n",
    "                        new_weight_name = 'kernel:0'\n",
    "                    elif '_b_' in dataset_name:\n",
    "                        new_weight_name = 'bias:0'\n",
    "                    else:\n",
    "                        new_weight_name = dataset_name\n",
    "                    layer_group_out.create_dataset(new_weight_name, data=data)\n",
    "            elif layer_name.startswith('decoded_mean'):\n",
    "                # 处理 decoded_mean 层\n",
    "                layer_group_in = model_weights_group[layer_name]\n",
    "                layer_group_out = f_out.create_group(layer_name)\n",
    "                for dataset_name in layer_group_in:\n",
    "                    data = layer_group_in[dataset_name][()]\n",
    "                    if '_W_' in dataset_name:\n",
    "                        new_weight_name = 'kernel:0'\n",
    "                    elif '_b_' in dataset_name:\n",
    "                        new_weight_name = 'bias:0'\n",
    "                    else:\n",
    "                        new_weight_name = dataset_name\n",
    "                    layer_group_out.create_dataset(new_weight_name, data=data)\n",
    "            elif layer_name.startswith('flatten'):\n",
    "                # 处理 flatten 层（无权重）\n",
    "                layer_group_in = model_weights_group[layer_name]\n",
    "                f_out.create_group(layer_name)\n",
    "            elif layer_name.startswith('repeat_vector'):\n",
    "                # 处理 repeat_vector 层（无权重）\n",
    "                layer_group_in = model_weights_group[layer_name]\n",
    "                f_out.create_group(layer_name)\n",
    "            elif layer_name.startswith('lambda'):\n",
    "                # 处理 lambda 层（无权重）\n",
    "                layer_group_in = model_weights_group[layer_name]\n",
    "                f_out.create_group(layer_name)\n",
    "            else:\n",
    "                # 其他层按原样复制\n",
    "                layer_group_in = model_weights_group[layer_name]\n",
    "                layer_group_out = f_out.create_group(layer_name)\n",
    "                for dataset_name in layer_group_in:\n",
    "                    data = layer_group_in[dataset_name][()]\n",
    "                    layer_group_out.create_dataset(dataset_name, data=data)\n",
    "                \n",
    "        # 注意：optimizer_weights 组已被忽略\n",
    "        print(\"Weights adjustment completed.\")\n",
    "\n",
    "# 使用示例：\n",
    "adjust_weights(\n",
    "    '/mnt/d/Research/PHD/DLEPS/data/zinc_vae_grammar_L56_E100_val.hdf5', \n",
    "    '/mnt/d/Research/PHD/DLEPS/data/adjusted_weights.hdf5'\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dleps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
