{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/mnt/d/Research/PHD/DLEPS/code/DLEPS')\n",
    "\n",
    "from rdkit.Chem import MolFromSmiles, MolToSmiles\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import molecule_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取训练和测试的 SMILES 数据\n",
    "dt1 = pd.read_csv('/mnt/d/Research/PHD/DLEPS/results/train_SMILES_demo.csv')\n",
    "dt2 = pd.read_csv('/mnt/d/Research/PHD/DLEPS/results/test_SMILES_demo.csv')\n",
    "\n",
    "# 合并 SMILES 列表\n",
    "smiles = np.concatenate([dt1['smiles'].values, dt2['smiles'].values], axis=0)\n",
    "\n",
    "print(\"Number of SMILES from dt1 and dt2:\", len(smiles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取 L1000 基因表达数据\n",
    "l1000_df = pd.read_csv('/mnt/d/Research/PHD/DLEPS/results/L1000_landmark.csv')\n",
    "\n",
    "print(\"Number of SMILES in L1000_landmark.csv:\", len(l1000_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查 L1000 数据的列\n",
    "print(l1000_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于 SMILES 可能存在不同的表示方式，我们需要将它们规范化为标准的 SMILES\n",
    "# 处理合并的 SMILES 数据\n",
    "canonical_smiles = []\n",
    "for smi in smiles:\n",
    "    try:\n",
    "        mol = MolFromSmiles(smi)\n",
    "        if mol is not None:\n",
    "            can_smi = MolToSmiles(mol)\n",
    "            canonical_smiles.append(can_smi)\n",
    "        else:\n",
    "            canonical_smiles.append(None)\n",
    "    except:\n",
    "        canonical_smiles.append(None)\n",
    "\n",
    "# 创建 SMILES DataFrame\n",
    "smiles_df = pd.DataFrame({'smiles': smiles, 'canonical_smiles': canonical_smiles})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理 L1000 基因表达数据的 SMILES 列\n",
    "canonical_smiles_l1000 = []\n",
    "for smi in l1000_df['smiles']:\n",
    "    try:\n",
    "        mol = MolFromSmiles(smi)\n",
    "        if mol is not None:\n",
    "            can_smi = MolToSmiles(mol)\n",
    "            canonical_smiles_l1000.append(can_smi)\n",
    "        else:\n",
    "            canonical_smiles_l1000.append(None)\n",
    "    except:\n",
    "        canonical_smiles_l1000.append(None)\n",
    "\n",
    "l1000_df['canonical_smiles'] = canonical_smiles_l1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重置索引以便后续的合并\n",
    "smiles_df.reset_index(inplace=True)\n",
    "smiles_df.rename(columns={'index': 'smiles_index'}, inplace=True)\n",
    "\n",
    "l1000_df.reset_index(inplace=True)\n",
    "l1000_df.rename(columns={'index': 'l1000_index'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并两个数据集，基于规范化后的 SMILES\n",
    "merged_df = pd.merge(smiles_df, l1000_df, on='canonical_smiles', how='inner', suffixes=('_smiles', '_l1000'))\n",
    "\n",
    "print(\"Number of matched SMILES after merging:\", len(merged_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取匹配的索引和基因表达数据\n",
    "matched_indices = merged_df['smiles_index'].values\n",
    "L962 = merged_df.iloc[:, merged_df.columns.get_loc('780'):].values  # 假设基因表达数据从列名 '780' 开始\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取需要处理的 SMILES\n",
    "smiles_to_process = merged_df['smiles_smiles'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理 SMILES，转换为 RDKit 标准 SMILES，并记录有效的索引\n",
    "smiles_rdkit = []\n",
    "iid = []\n",
    "for i, smi in enumerate(smiles_to_process):\n",
    "    try:\n",
    "        mol = MolFromSmiles(smi)\n",
    "        if mol is not None:\n",
    "            can_smi = MolToSmiles(mol)\n",
    "            smiles_rdkit.append(can_smi)\n",
    "            iid.append(i)\n",
    "        else:\n",
    "            print(\"Invalid molecule at index %d\" % i)\n",
    "    except:\n",
    "        print(\"Error processing SMILES at index %d\" % i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of valid SMILES after RDKit processing:\", len(smiles_rdkit))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新基因表达数据，保留有效的 SMILES 对应的数据\n",
    "L962_valid = L962[iid]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义辅助函数\n",
    "def xlength(y):\n",
    "    from functools import reduce\n",
    "    return reduce(lambda sum, element: sum + 1, y, 0)\n",
    "\n",
    "def get_zinc_tokenizer(cfg):\n",
    "    long_tokens = [a for a in list(cfg._lexical_index.keys()) if xlength(a) > 1]\n",
    "    replacements = ['$','%','^']\n",
    "    assert xlength(long_tokens) == len(replacements)\n",
    "    for token in replacements: \n",
    "        assert token not in cfg._lexical_index\n",
    "\n",
    "    def tokenize(smiles):\n",
    "        for i, token in enumerate(long_tokens):\n",
    "            smiles = smiles.replace(token, replacements[i])\n",
    "        tokens = []\n",
    "        for token in smiles:\n",
    "            try:\n",
    "                ix = replacements.index(token)\n",
    "                tokens.append(long_tokens[ix])\n",
    "            except:\n",
    "                tokens.append(token)\n",
    "        return tokens\n",
    "\n",
    "    return tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zinc_grammar\n",
    "import nltk\n",
    "\n",
    "_tokenize = get_zinc_tokenizer(zinc_grammar.GCFG)\n",
    "_parser = nltk.ChartParser(zinc_grammar.GCFG)\n",
    "_productions = zinc_grammar.GCFG.productions()\n",
    "_prod_map = {}\n",
    "for ix, prod in enumerate(_productions):\n",
    "    _prod_map[prod] = ix\n",
    "MAX_LEN = 277\n",
    "_n_chars = len(_productions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对 SMILES 进行解析和编码\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "def parse_smiles_with_index(args):\n",
    "    \"\"\"Parse SMILES token with its index.\"\"\"\n",
    "    i, t = args\n",
    "    try:\n",
    "        tp = next(_parser.parse(t))\n",
    "        return (i, tp, None)  # 返回成功解析的索引和解析树\n",
    "    except Exception as e:\n",
    "        return (i, None, str(e))  # 返回失败索引和错误信息\n",
    "\n",
    "# 使用并行处理\n",
    "if __name__ == \"__main__\":\n",
    "    tokens = list(map(_tokenize, smiles_rdkit))\n",
    "    parse_trees = []\n",
    "    badi = []\n",
    "\n",
    "    # 使用进程池进行并行化\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        results = pool.map(parse_smiles_with_index, enumerate(tokens))\n",
    "\n",
    "    # 处理结果\n",
    "    for i, tp, error in results:\n",
    "        if tp is not None:\n",
    "            parse_trees.append(tp)\n",
    "        else:\n",
    "            print(f\"Parse tree error at index {i}: {error}\")\n",
    "            badi.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新有效的索引，排除解析错误的 SMILES\n",
    "iid2 = [iid[i] for i in range(len(iid)) if i not in badi]\n",
    "L962_valid = L962_valid[[i for i in range(len(L962_valid)) if i not in badi]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成 One-Hot 编码\n",
    "productions_seq = [tree.productions() for tree in parse_trees]\n",
    "indices = [np.array([_prod_map[prod] for prod in entry], dtype=int) for entry in productions_seq]\n",
    "one_hot = np.zeros((len(indices), MAX_LEN, _n_chars), dtype=np.float32)\n",
    "for i in range(len(indices)):\n",
    "    num_productions = len(indices[i])\n",
    "    if num_productions > MAX_LEN:\n",
    "        print(\"Too large molecule at index %d, truncating\" % i)\n",
    "        one_hot[i][np.arange(MAX_LEN), indices[i][:MAX_LEN]] = 1.\n",
    "    else:\n",
    "        one_hot[i][np.arange(num_productions), indices[i]] = 1.\n",
    "        one_hot[i][np.arange(num_productions, MAX_LEN), -1] = 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查处理后的数据大小\n",
    "print(\"Size of one-hot encoded SMILES:\", one_hot.shape)\n",
    "print(\"Size of gene expression data:\", L962_valid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机打乱并划分训练和测试集\n",
    "num_examples = L962_valid.shape[0]\n",
    "perm = np.arange(num_examples)\n",
    "np.random.shuffle(perm)\n",
    "L962_shuffled = L962_valid[perm]\n",
    "one_hot_shuffled = one_hot[perm]\n",
    "\n",
    "TEST_SIZE = 3000\n",
    "L962_test = L962_shuffled[:TEST_SIZE]\n",
    "L962_train = L962_shuffled[TEST_SIZE:]\n",
    "one_hot_test = one_hot_shuffled[:TEST_SIZE]\n",
    "one_hot_train = one_hot_shuffled[TEST_SIZE:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存数据为 .h5 文件\n",
    "import h5py\n",
    "\n",
    "# 保存基因表达数据\n",
    "h5f = h5py.File('/mnt/d/Research/PHD/DLEPS/results/L1000_train.h5', 'w')\n",
    "h5f.create_dataset('data', data=L962_train)\n",
    "h5f.close()\n",
    "\n",
    "h5f = h5py.File('/mnt/d/Research/PHD/DLEPS/results/L1000_test.h5', 'w')\n",
    "h5f.create_dataset('data', data=L962_test)\n",
    "h5f.close()\n",
    "\n",
    "# 保存 One-Hot 编码的 SMILES\n",
    "h5f = h5py.File('/mnt/d/Research/PHD/DLEPS/results/SMILE_train_demo.h5', 'w')\n",
    "h5f.create_dataset('data', data=one_hot_train)\n",
    "h5f.close()\n",
    "\n",
    "h5f = h5py.File('/mnt/d/Research/PHD/DLEPS/results/SMILE_test_demo.h5', 'w')\n",
    "h5f.create_dataset('data', data=one_hot_test)\n",
    "h5f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dleps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
