{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import sys\n",
    "# 将自定义模块路径添加到系统路径中\n",
    "sys.path.append('../DLEPS/')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from rdkit.Chem import MolFromSmiles, MolToSmiles, Draw\n",
    "import molecule_vae\n",
    "import zinc_grammar\n",
    "import nltk\n",
    "from functools import reduce\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取训练集和测试集的 SMILES 数据\n",
    "train_smiles_df = pd.read_csv('../../results/train_SMILES_demo.csv')\n",
    "test_smiles_df = pd.read_csv('../../results/test_SMILES_demo.csv')\n",
    "\n",
    "# 合并训练集和测试集的 SMILES 数据\n",
    "combined_smiles = np.concatenate([train_smiles_df['smiles'].values, test_smiles_df['smiles'].values], axis=0)\n",
    "\n",
    "print(\"Number of SMILES from train and test datasets:\", len(combined_smiles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取 L1000 基因表达数据\n",
    "gene_expression_df = pd.read_csv('../../results/L1000_landmark.csv')\n",
    "\n",
    "print(\"Number of SMILES in L1000_landmark.csv:\", len(gene_expression_df))\n",
    "print(\"Columns in gene expression data:\", gene_expression_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 规范化合并后的 SMILES 数据为标准 SMILES\n",
    "def normalize_smiles(smiles_list):\n",
    "    normalized_smiles = []\n",
    "    for smi in smiles_list:\n",
    "        try:\n",
    "            molecule = MolFromSmiles(smi)\n",
    "            if molecule:\n",
    "                canonical_smi = MolToSmiles(molecule)\n",
    "                normalized_smiles.append(canonical_smi)\n",
    "            else:\n",
    "                normalized_smiles.append(None)\n",
    "        except Exception as e:\n",
    "            normalized_smiles.append(None)\n",
    "    return normalized_smiles\n",
    "\n",
    "# 规范化 SMILES\n",
    "canonical_smiles = normalize_smiles(combined_smiles)\n",
    "\n",
    "# 创建包含原始和规范化 SMILES 的 DataFrame\n",
    "smiles_df = pd.DataFrame({\n",
    "    'original_smiles': combined_smiles,\n",
    "    'canonical_smiles': canonical_smiles\n",
    "})\n",
    "\n",
    "# 显示规范化结果\n",
    "print(smiles_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 规范化基因表达数据中的 SMILES\n",
    "gene_expression_canonical_smiles = normalize_smiles(gene_expression_df['smiles'])\n",
    "\n",
    "# 将规范化后的 SMILES 添加为新列\n",
    "gene_expression_df['canonical_smiles'] = gene_expression_canonical_smiles\n",
    "\n",
    "# 显示规范化结果\n",
    "print(gene_expression_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重置索引以便合并\n",
    "smiles_df.reset_index(inplace=True, drop=True)\n",
    "gene_expression_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# 合并两个数据集，基于规范化后的 SMILES\n",
    "merged_df = pd.merge(\n",
    "    smiles_df,\n",
    "    gene_expression_df,\n",
    "    on='canonical_smiles',\n",
    "    how='inner',\n",
    "    suffixes=('_smiles', '_gene')\n",
    ")\n",
    "\n",
    "print(\"Number of matched SMILES after merging:\", len(merged_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取匹配的 SMILES 索引\n",
    "matched_smiles_indices = merged_df.index.values\n",
    "\n",
    "# 假设基因表达数据从列名 '780' 开始，提取基因表达数据\n",
    "gene_expression_columns = merged_df.columns[merged_df.columns.get_loc('780'):]\n",
    "gene_expression_data = merged_df[gene_expression_columns].values\n",
    "\n",
    "# 提取需要处理的 SMILES\n",
    "smiles_to_process = merged_df['original_smiles'].values\n",
    "\n",
    "print(\"Extracted gene expression data shape:\", gene_expression_data.shape)\n",
    "print(\"Number of SMILES to process:\", len(smiles_to_process))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 RDKit 进一步处理 SMILES，记录有效的索引\n",
    "processed_smiles = []\n",
    "valid_indices = []\n",
    "\n",
    "for idx, smi in enumerate(smiles_to_process):\n",
    "    try:\n",
    "        molecule = MolFromSmiles(smi)\n",
    "        if molecule:\n",
    "            canonical_smi = MolToSmiles(molecule)\n",
    "            processed_smiles.append(canonical_smi)\n",
    "            valid_indices.append(idx)\n",
    "        else:\n",
    "            print(f\"Invalid molecule at index {idx}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing SMILES at index {idx}: {e}\")\n",
    "\n",
    "print(\"Number of valid SMILES after RDKit processing:\", len(processed_smiles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过滤基因表达数据，仅保留有效 SMILES 对应的数据\n",
    "valid_gene_expression_data = gene_expression_data[valid_indices]\n",
    "\n",
    "print(\"Filtered gene expression data shape:\", valid_gene_expression_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义计算对象长度的辅助函数\n",
    "def calculate_length(sequence):\n",
    "    return reduce(lambda total, _: total + 1, sequence, 0)\n",
    "\n",
    "# 定义 ZINC 分词器函数\n",
    "def get_zinc_tokenizer(cfg):\n",
    "    # 提取长度大于1的长分子符号\n",
    "    long_tokens = [token for token in cfg._lexical_index.keys() if calculate_length(token) > 1]\n",
    "    \n",
    "    # 定义替换字符\n",
    "    replacements = ['$', '%', '^']\n",
    "    \n",
    "    # 确保替换字符数量与长分子符号数量一致\n",
    "    assert calculate_length(long_tokens) == len(replacements), \"Mismatch between long tokens and replacements.\"\n",
    "    \n",
    "    # 确保替换字符未被占用\n",
    "    for token in replacements:\n",
    "        assert token not in cfg._lexical_index, f\"Replacement token {token} already in lexical index.\"\n",
    "    \n",
    "    def tokenize(smiles):\n",
    "        # 替换长分子符号为单字符\n",
    "        for i, token in enumerate(long_tokens):\n",
    "            smiles = smiles.replace(token, replacements[i])\n",
    "        \n",
    "        tokens = []\n",
    "        for char in smiles:\n",
    "            try:\n",
    "                # 尝试还原长分子符号\n",
    "                index = replacements.index(char)\n",
    "                tokens.append(long_tokens[index])\n",
    "            except ValueError:\n",
    "                # 保留原字符\n",
    "                tokens.append(char)\n",
    "        return tokens\n",
    "    \n",
    "    return tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化分词器和解析器\n",
    "tokenizer = get_zinc_tokenizer(zinc_grammar.GCFG)\n",
    "parser = nltk.ChartParser(zinc_grammar.GCFG)\n",
    "productions = zinc_grammar.GCFG.productions()\n",
    "\n",
    "# 创建产生式规则到索引的映射\n",
    "production_map = {prod: idx for idx, prod in enumerate(productions)}\n",
    "\n",
    "# 设置编码参数\n",
    "MAX_SMILES_LENGTH = 277\n",
    "NUM_PRODUCTIONS = len(productions)\n",
    "\n",
    "print(f\"Number of productions: {NUM_PRODUCTIONS}\")\n",
    "print(f\"Maximum SMILES length: {MAX_SMILES_LENGTH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义函数，用于解析 SMILES 并返回解析树\n",
    "def parse_smiles(args):\n",
    "    index, tokens = args\n",
    "    try:\n",
    "        parse_tree = next(parser.parse(tokens))\n",
    "        return (index, parse_tree, None)\n",
    "    except Exception as e:\n",
    "        return (index, None, str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对所有 SMILES 进行分词\n",
    "tokenized_smiles = list(map(tokenizer, processed_smiles))\n",
    "\n",
    "# 使用多进程并行解析 SMILES\n",
    "with Pool(cpu_count()) as pool:\n",
    "    parse_results = pool.map(parse_smiles, enumerate(tokenized_smiles))\n",
    "\n",
    "# 处理解析结果，收集成功的解析树和失败的索引\n",
    "parse_trees = []\n",
    "failed_indices = []\n",
    "\n",
    "for idx, tree, error in parse_results:\n",
    "    if tree is not None:\n",
    "        parse_trees.append(tree)\n",
    "    else:\n",
    "        print(f\"Parse tree error at index {idx}: {error}\")\n",
    "        failed_indices.append(idx)\n",
    "\n",
    "print(\"Number of successfully parsed SMILES:\", len(parse_trees))\n",
    "print(\"Number of failed parses:\", len(failed_indices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过滤掉解析失败的 SMILES 对应的索引\n",
    "valid_parse_indices = [i for i in range(len(valid_indices)) if i not in failed_indices]\n",
    "\n",
    "# 更新有效的基因表达数据和 SMILES\n",
    "final_gene_expression_data = valid_gene_expression_data[valid_parse_indices]\n",
    "final_smiles = [processed_smiles[i] for i in valid_parse_indices]\n",
    "\n",
    "print(\"Final gene expression data shape:\", final_gene_expression_data.shape)\n",
    "print(\"Number of final SMILES to encode:\", len(final_smiles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取每个解析树的产生式规则序列\n",
    "production_sequences = [tree.productions() for tree in parse_trees]\n",
    "\n",
    "# 将产生式规则映射为索引序列\n",
    "production_indices = [\n",
    "    np.array([production_map[prod] for prod in seq], dtype=int) \n",
    "    for seq in production_sequences\n",
    "]\n",
    "\n",
    "# 初始化 One-Hot 编码矩阵\n",
    "one_hot_encoded = np.zeros((len(production_indices), MAX_SMILES_LENGTH, NUM_PRODUCTIONS), dtype=np.float32)\n",
    "\n",
    "# 填充 One-Hot 编码矩阵\n",
    "for i, indices in enumerate(production_indices):\n",
    "    num_productions = len(indices)\n",
    "    if num_productions > MAX_SMILES_LENGTH:\n",
    "        print(f\"SMILES at index {i} exceeds max length, truncating.\")\n",
    "        one_hot_encoded[i, :MAX_SMILES_LENGTH, indices[:MAX_SMILES_LENGTH]] = 1.0\n",
    "    else:\n",
    "        one_hot_encoded[i, :num_productions, indices] = 1.0\n",
    "        one_hot_encoded[i, num_productions:, -1] = 1.0  # 填充空白位置\n",
    "\n",
    "print(\"One-Hot encoded SMILES shape:\", one_hot_encoded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子以确保可重复性\n",
    "np.random.seed(42)\n",
    "\n",
    "# 获取样本数量\n",
    "num_samples = final_gene_expression_data.shape[0]\n",
    "\n",
    "# 生成随机排列的索引\n",
    "shuffled_indices = np.random.permutation(num_samples)\n",
    "\n",
    "# 打乱基因表达数据和 One-Hot 编码数据\n",
    "shuffled_gene_expression = final_gene_expression_data[shuffled_indices]\n",
    "shuffled_one_hot = one_hot_encoded[shuffled_indices]\n",
    "\n",
    "# 定义测试集大小\n",
    "TEST_SET_SIZE = 3000\n",
    "\n",
    "# 划分测试集和训练集\n",
    "gene_expression_test = shuffled_gene_expression[:TEST_SET_SIZE]\n",
    "gene_expression_train = shuffled_gene_expression[TEST_SET_SIZE:]\n",
    "\n",
    "one_hot_test = shuffled_one_hot[:TEST_SET_SIZE]\n",
    "one_hot_train = shuffled_one_hot[TEST_SET_SIZE:]\n",
    "\n",
    "print(\"Training set gene expression shape:\", gene_expression_train.shape)\n",
    "print(\"Training set One-Hot shape:\", one_hot_train.shape)\n",
    "print(\"Test set gene expression shape:\", gene_expression_test.shape)\n",
    "print(\"Test set One-Hot shape:\", one_hot_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义保存数据的函数\n",
    "def save_to_h5(file_path, dataset_name, data):\n",
    "    with h5py.File(file_path, 'w') as h5f:\n",
    "        h5f.create_dataset(dataset_name, data=data)\n",
    "    print(f\"Data saved to {file_path} with dataset name '{dataset_name}'.\")\n",
    "\n",
    "# 保存基因表达训练集和测试集\n",
    "save_to_h5('../../results/L1000_train.h5', 'data', gene_expression_train)\n",
    "save_to_h5('../../results/L1000_test.h5', 'data', gene_expression_test)\n",
    "\n",
    "# 保存 One-Hot 编码的 SMILES 训练集和测试集\n",
    "save_to_h5('../../results/SMILES_train_demo.h5', 'data', one_hot_train)\n",
    "save_to_h5('../../results/SMILES_test_demo.h5', 'data', one_hot_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLEPS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
